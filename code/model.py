"""
Docstring for model.py
"""

import numpy as np
import torch
import torch.nn as nn
from tqdm import tqdm

import dataset
from encoder import Encoder
from decoder import Decoder
from config import configurate
from config import config


def main(conf):
    # print(conf)

    model = Model(conf)
    model.to(conf['device'])
    loader = torch.utils.data.DataLoader(model.data, batch_size=conf['sentence_batch_size'], collate_fn=collate_batch)
    progress_bar = tqdm(enumerate(loader), disable=True)
    for _, (words_batch, chars_batch, labels_batch) in progress_bar:
        tags, loss = model(words_batch, chars_batch, labels_batch)


class Model(nn.Module):
    """
    Class takes batches produced by DataLoader and assignes embeddings to words using WordEmbeddings class.
    Creates an instance of CustomDataset.

    Parameters
    ----------
    conf : dict
        Dictionary with configuration parameters
    Examples
    --------
        model = Model(conf) \n
        loader = torch.utils.data.DataLoader(model.data, batch_size=conf['sentence_batch_size'], collate_fn=collate_batch) \n
        for (words_batch, chars_batch, labels_batch) in loader:
            logits, loss = model(words_batch, chars_batch, labels_batch)
    """

    def __init__(self, conf):
        super().__init__()
        self.conf = conf
        self.data = dataset.CustomDataset(self.conf)
        self.encoder = Encoder(self.conf, self.data) # provides words embeddings
        self.decoder = Decoder(self.conf, self.data)
        self.grammeme_embeddings = None

        self.loss = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')
        self.optimizer = torch.optim.SGD(self.parameters(), lr=self.conf['learning_rate'])

    def forward(self, words_batch, chars_batch, labels_batch):
        """
        Parameters
        ----------
        words_batch : torch.Tensor
            Tensor of words indices for every word in a batch. Size (max_sentence_length, batch_size)
        chars_batch : torch.Tensor
            Tensor of chars indices for every word in a batch. Size (batch_size * max_sentence_length, max_word_length)
        labels_batch : torch.Tensor
            Tensor of labels indices for every word in a batch. Size (max_label_length, batch_size * max_sentence_length)
        """

        encoder_hidden, encoder_cell = self.encoder(words_batch, chars_batch) # shape (max_sentence_length, batch_size, grammeme_LSTM_hidden)
        decoder_hidden = encoder_hidden.view(-1, encoder_hidden.size(dim=2))
        decoder_cell = encoder_cell.view(-1, encoder_cell.size(dim=2))
        predictions, probabilities = self.decoder(labels_batch, decoder_hidden, decoder_cell)
        tags = self.predictions_to_grammemes(predictions)

        targets = torch.flatten(labels_batch).to(torch.long)
        probabilities = torch.flatten(probabilities, start_dim=0, end_dim=1)
        loss = self.loss(probabilities, targets)
        loss.backward()
        self.optimizer.step()
        print(loss)

        return tags, loss

    def predictions_to_grammemes(self, predictions):
        """
        Turns indices of predictions produced by decoder into actual grammemes (strings)
        Parameters
        ----------
        predictions : torch.Tensor
            2D Tensor containing indices
        Returns
        -------
        list
            List of lists of predicions
        """
        tags = []
        for tag_indices in predictions:
            tag = []
            for grammeme_index in tag_indices:
                tag += [self.data.vocab.vocab['index-grammeme'][grammeme_index.item()]]
            tags += [tag]
        return tags
    # train - separate function that has instance of dataloader

    # model produces vector of size (n_grammemes) that will be compared to one-hot representation of the labels

def collate_batch(batch, pad_id=0, sos_id=1, eos_id=2): # do all preprocessing here
    """
    Function takes batch created with CustomDataset and performs padding. batch consists of indices of words,
    chars, and labels (grammemes). The returned batches are tensors.

    Parameters
    ---------
    batch : list
        A single batch to be collated. Batch consists of tuples (words, labels) generated by CustomDataset
    pad_id : int, default 0
        The id of the pad token in all dictionaries
    sos_id : int, default 1
        The id of the sos (start of sequence) token
    eos_id : int, default 2
        The id of the eos (end of sequence) token
    Returns
    -------
    tuple
        (words_batch, chars_batch, labels_batch). All of them have type torch.Tensor. Size of words_batch is (max_sentence_length, batch_size).
        Size of chars_batch is (batch_size * max_sentence_length, max_word_length). Size of labels_batch is
        (max_label_length, batch_size * max_sentence_length)
    """

    sentences = [element[0] for element in batch] # sentences is a list of all list of words
    tags = [element[1] for element in batch] # tags is a list of all lists of grammemes
    max_sentence_length = max(map(lambda x: len(x), sentences))
    max_word_length = max([max(map(lambda x: len(x), sentence)) for sentence in sentences])
    max_label_length = 2 + max([max(map(lambda x: len(x), tag)) for tag in tags]) # +2 because of the eos token
    words_batch = []
    chars_batch = []
    labels_batch = []
    for words, labels in batch:
        words_indices = []
        chars_indices = []
        labels_indices = []
        for word in words:
            word += [pad_id] * (max_word_length - len(word)) # id of the pad token must be 0
            words_indices += [word[0]]
            chars_indices += [word[1:]]
        words_indices += [pad_id] * (max_sentence_length - len(words))
        chars_indices += [[pad_id] * (max_word_length - 1)] * (max_sentence_length - len(words))

        for label in labels:
            label.insert(0, sos_id) # id of the sos token must be 1
            label += [eos_id] # id of the eos token must be 2
            label += [pad_id] * (max_label_length - len(label))
            labels_indices += [label]
        labels_indices += [[pad_id] * max_label_length] * (max_sentence_length - len(words))

        words_batch += [words_indices]
        chars_batch += [chars_indices]
        labels_batch += [labels_indices]

    words_batch = torch.tensor(words_batch, dtype=torch.int)
    words_batch = words_batch.transpose(1, 0)
    chars_batch = torch.tensor(chars_batch, dtype=torch.int)
    chars_batch = chars_batch.view(-1, chars_batch.shape[2])
    labels_batch = torch.tensor(labels_batch, dtype=torch.int)
    labels_batch = labels_batch.view(-1, labels_batch.shape[2]).permute(1, 0)
    return words_batch.to(config['device']), chars_batch.to(config['device']), labels_batch.to(config['device'])


if __name__ == "__main__":
    configurate()
    main(config)
