"""
Docstring for model.py
"""

import numpy as np
import torch
import torch.nn as nn
from tqdm import tqdm

import dataset
from encoder import Encoder
from config import configurate
from config import config

# all layers and classes should be descendants of nn.Module and all of them should have forward()
# multitask learning

def main(conf):
    # print(conf)
    model = Model(conf)
    loader = torch.utils.data.DataLoader(model.data, batch_size=conf['sentence_batch_size'], collate_fn=collate_batch)
    progress_bar = tqdm(enumerate(loader))
    for _, (words_batch, chars_batch, labels_batch) in progress_bar:
        logits, loss = model(words_batch, chars_batch, labels_batch)


class Model(nn.Module): # for now, it is here, maybe move it elsewhere
    """
    Class takes batches produced by DataLoader and assignes embeddings to words using WordEmbeddings class.
    Creates an instance of CustomDataset.

    Parameters
    ----------
    conf : dict
        Dictionary with configuration parameters
    Examples
    --------
        model = Model(conf) \n
        loader = torch.utils.data.DataLoader(model.data, batch_size=conf['sentence_batch_size'], collate_fn=collate_batch) \n
        for (words_batch, chars_batch, labels_batch) in loader:
            logits, loss = model(words_batch, chars_batch, labels_batch)
    """

    def __init__(self, conf):
        super().__init__()
        self.conf = conf
        self.data = dataset.CustomDataset(self.conf)
        self.word_embeddings = Encoder(self.conf, self.data)
        self.grammeme_embeddings = None

    def forward(self, words_batch, chars_batch, labels_batch):
        """
        Parameters
        ----------
        words_batch : torch.Tensor
            Tensor of words indices for every word in a batch. Size (batch_size, max_sentence_length)
        chars_batch : torch.Tensor
            Tensor of chars indices for every word in a batch. Size (batch_size * max_sentence_length, max_word_length)
        """

        words = self.word_embeddings(words_batch, chars_batch)
        logits = 0
        loss = 0
        return logits, loss
    # train - separate function that has instance of dataloader

    # model produces vector of size (n_grammemes) that will be compared to one-hot representation of the labels
    # 150 (grammeme_LSTM_hidden) is used in lstm
    # in sequential model, we find the index of max value and pass it forward

def collate_batch(batch, pad_id=0, eos_id=2): # do all preprocessing here
    """
    Function takes batch created with CustomDataset and performs padding. batch consists of indices of words,
    chars, and labels (grammemes). The returned batches are tensors.

    Parameters
    ---------
    batch : list
        A single batch to be collated. Batch consists of tuples (words, labels) generated by CustomDataset
    pad_id : int, default 0
        The id of the pad token in all dictionaries
    eos_id : int, default 2
        The id of the eos token
    Returns
    -------
    tuple
        (words_batch, chars_batch, labels_batch). words_batch is a torch.Tensor and has size (batch_size, max_sentence_length).
        chars_batch is a torch.Tensor and has size (batch_size * max_sentence_length, max_word_length).
    """

    sentences = [element[0] for element in batch] # sentences is a list of all list of words
    max_sentence_length = max(map(lambda x: len(x), sentences))
    max_word_length = max([max(map(lambda x: len(x), sentence)) for sentence in sentences])
    words_batch = []
    chars_batch = []
    labels_batch = []
    for words, labels in batch:
        words_indices = []
        chars_indices = []
        for word in words:
            word += [pad_id] * (max_word_length - len(word)) # id of the pad token must be 0
            words_indices += [word[0]]
            chars_indices += [word[1:]]
        words_indices += [pad_id] * (max_sentence_length - len(words))
        chars_indices += [[pad_id] * (max_word_length - 1)] * (max_sentence_length - len(words))

        for grammemes in labels:
            grammemes += [eos_id, pad_id] # id of the eos token must be 2

        words_batch += [words_indices]
        chars_batch += [chars_indices]
        labels_batch += [labels]

    words_batch = torch.tensor(words_batch, dtype=torch.int)
    chars_batch = torch.tensor(chars_batch, dtype=torch.int)
    chars_batch = chars_batch.view(-1, chars_batch.shape[2])
    return words_batch, chars_batch, labels_batch


if __name__ == "__main__":
    configurate()
    main(config)
