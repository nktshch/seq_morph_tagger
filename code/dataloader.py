"""
Docstring for dataloader.py
"""

import numpy as np
import torch
import torch.nn as nn
from tqdm import tqdm

import dataset
from config import configurate
from config import config

# all layers and classes should be descendants of nn.Module and all of them should have forward()
# multitask learning

def main(conf):
    # print(conf)
    model = Model(conf)
    loader = torch.utils.data.DataLoader(model.data, batch_size=conf['sentence_batch_size'], collate_fn=collate_batch)
    progress_bar = tqdm(enumerate(loader))
    for iteration, batch in progress_bar:
        for words, labels in batch:
            logits, loss = model(words, labels)

class WordEmbeddings(nn.Module):
    def __init__(self, conf, data):
        super().__init__()
        self.conf = conf
        self.data = data
        self.word_embeddings = nn.Embedding.from_pretrained(torch.from_numpy(data.embeddings))
        self.charLSTM = nn.LSTM(input_size=self.conf['char_embeddings_dimension'], hidden_size=self.conf['char_embeddings_hidden'], bidirectional=True)
        self.char_embeddings = nn.Embedding(len(data.vocab.vocab['char-index']), self.conf["char_embeddings_dimension"])

    def forward(self, words_idx):
        words_indices = []
        chars_indices = []
        for indices in words_idx:
            words_indices += [indices[0]]
            chars_indices += [indices[1:]]
        words = self.word_embeddings(torch.Tensor(words_indices).int())
        chars = self.char_embeddings(torch.Tensor([index for word in chars_indices for index in word]).int())
        chars, (hn, cn) = self.charLSTM(chars)
        chars = torch.reshape(chars, (len(words_idx), len(chars_indices[0]), 300)) # my mistake here! have to handle all of it differently because of how LSTM module works
        chars = torch.mean(chars, dim=1)
        words = torch.cat((words, chars), dim=1)
        return words, chars

class Model(nn.Module): # for now, it is here, maybe move it elsewhere
    def __init__(self, conf):
        super().__init__()
        self.conf = conf
        self.data = dataset.CustomDataset(self.conf)
        self.word_embeddings = WordEmbeddings(self.conf, self.data)
        self.grammeme_embeddings = None
        self.batch_size = conf["sentence_batch_size"]

    def forward(self, words_idx, labels_idx):
        # words_idx and labels_idx are already for a single sentence, not for a batch (see main())
        words = self.word_embeddings(words_idx)
        logits = 0
        loss = 0
        return logits, loss
    # train - separate function that has instance of dataloader

    # model produces vector of size (n_grammemes) that will be compared to one-hot representation of the labels
    # 150 (grammeme_embeddings_hidden) is used in lstm
    # in sequential model, we find the index of max value and pass it forward

def collate_batch(batch, pad_id=0, eos_id=2):
    """
    Function takes batch created with CustomDataset and performs padding

    Parameters
    ---------
    batch : list
        A single batch to be collated. Batch consists of tuples (words, labels) generated by CustomDataset
    pad_id : int, default 0
        The id of the pad token in all dictionaries
    eos_id : int, default 2
        The id of the eos token
    """
    sentences = [element[0] for element in batch] # sentences is a list of all list of words
    max_sentence_length = max(map(lambda x: len(x), sentences))
    max_word_length = max([max(map(lambda x: len(x), sentence)) for sentence in sentences])
    for words, labels in batch:
        for word in words:
            word += [pad_id] * (max_word_length - len(word)) # id of the pad token must be 0
        words += [[pad_id] * max_word_length] * (max_sentence_length - len(words))

        for grammemes in labels:
            grammemes += [eos_id, pad_id] # id of the eos token must be 2
    return batch
    # one-hot encoding for chars
    # return torch.utils.data.default_collate(batch)


if __name__ == "__main__":
    configurate()
    main(config)
