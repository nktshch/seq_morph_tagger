"""
Docstring for dataloader.py
"""

import torch
import torch.utils.data

import dataset
from config import configurate
from config import config

def main(conf):
    print(conf)

    example_sentences = [66, 23, 12]
    data = dataset.CustomDataset(conf)
    batch = [data[i] for i in example_sentences]

class CustomDataloader:
    def __init__(self, conf, vocab, word_embeddings): # all of these is passed from CustomDataset
        self.conf = conf
        self.vocab = vocab
        self.word_embeddings = word_embeddings

        self.pad_id = vocab["word-index"][self.conf["PAD_word"]]  # all the pad tokens should have the same id
        self.eos_id = vocab["grammeme-index"][self.conf["EOS"]]

    def custom_collate(self, batch):
        """
        Function takes samples given by CustomDataset and creates batches to be fed into the model

        Parameters
        ---------
        batch : list
            A single batch to be collated. Batch consists of tuples (words, labels) generated by CustomDataset
        """

        max_sentence_length = max(map(lambda x: len(x), batch[:][0])) # batch[:][0] gives a list of all list of words
        max_word_length = max([max(map(lambda x: len(x), sentence)) for sentence in batch[:][0]])
        for words, labels in batch:
            for word in words:
                word += [self.pad_id] * (max_word_length - len(word))
            words += [self.pad_id] * (max_sentence_length - len(words))

            for grammemes in labels:
                grammemes += [self.eos_id, self.pad_id]

        # don't forget to turn this into tensors
        # return data.default_collate(batch)

    def tensor_from_ids(self, word, label):
        word_tensor = torch.tensor(self.word_embeddings[word[0]])

        # need to implement training for char_embeddings
        # char_tensors = [torch.tensor(char_embeddings[index]) for index in word[1:]]



if __name__ == "__main__":
    configurate()
    main(config)
