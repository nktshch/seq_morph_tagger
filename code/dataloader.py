"""
Docstring for dataloader.py
"""

import torch
import torch.nn as nn

import dataset
from config import configurate
from config import config

# all layers and classes should be descendants of nn.Module and all of them should have nn.forward

def main(conf):
    # print(conf)
    model = Model(conf)
    loader = torch.utils.data.DataLoader(model.data, batch_size=conf['sentence_batch_size'], collate_fn=collate_batch)
    for batch in loader:
        for sentence in batch:
            for index in range(len(sentence[1])):
                grammemes = [model.data.vocab.vocab['index-grammeme'][grammeme] for grammeme in sentence[1][index]]
                print(f"{model.data.vocab.vocab['index-word'][sentence[0][index][0]]} --- {grammemes}")
        break

class WordEmbeddings(nn.Module):
    def __init__(self, conf, data):
        super().__init__()
        self.word_embeddings = nn.Embedding.from_pretrained(torch.from_numpy(data.embeddings))
        self.charLSTM = nn.LSTM(input_size=conf['char_embeddings_dimension'], hidden_size=conf['char_embeddings_hidden'], bidirectional=True)
        self.char_embeddings = nn.Embedding(len(data.vocab.vocab['char-index']), conf["char_embeddings_dimension"])

    def forward(self):
        self.char_embeddings = self.charLSTM(self.char_embeddings)
        return self.word_embeddings, self.char_embeddings

class Model(nn.Module): # for now, it is here, maybe move it elsewhere
    def __init__(self, conf):
        super().__init__()
        self.conf = conf
        self.data = dataset.CustomDataset(self.conf)
        self.word_embeddings = WordEmbeddings(self.conf, self.data)
        self.grammeme_embeddings = None
        self.batch_size = conf["sentence_batch_size"]

    # train - separate function that has instance of dataloader

    # model produces vector of size (n_grammemes) that will be compared to one-hot representation of the labels
    # 150 (grammeme_embeddings_hidden) is used in lstm
    # in sequential model, we find the index of max value and pass it forward

    def print_batch(self, batch): # for debug purposes
        sentences = [element[0] for element in batch]
        for sentence in sentences:
            for word in sentence:
                word = [self.data.vocab.vocab["index-char"][char] for char in word[1:]]
                print(*word)

def collate_batch(batch):
    """
    Function takes batch created with CustomDataset and performs padding

    Parameters
    ---------
    batch : list
        A single batch to be collated. Batch consists of tuples (words, labels) generated by CustomDataset
    """
    sentences = [element[0] for element in batch] # sentences is a list of all list of words
    max_sentence_length = max(map(lambda x: len(x), sentences))
    max_word_length = max([max(map(lambda x: len(x), sentence)) for sentence in sentences])
    for words, labels in batch:
        for word in words:
            word += [0] * (max_word_length - len(word)) # id of the pad token must be 0
        words += [[0] * max_word_length] * (max_sentence_length - len(words))

        for grammemes in labels:
            grammemes += [2, 0] # id of the eos token must be 2
    return batch
    # one-hot encoding for chars
    # return torch.utils.data.default_collate(batch)


if __name__ == "__main__":
    configurate()
    main(config)
