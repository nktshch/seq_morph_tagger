"""
Docstring for dataloader.py
"""

import torch
import torch.nn as nn

import dataset
from config import configurate
from config import config

def main(conf):
    # print(conf)

    model = Model(conf)
    batch = model.batches(start=65)
    collated_batch = custom_collate(batch)
    model.print_batch(collated_batch) # debug


class Model: # for now, it is here, maybe move it elsewhere
    def __init__(self, conf):
        self.conf = conf
        self.data = dataset.CustomDataset(self.conf)
        self.word_embeddings = None
        self.char_embeddings = None
        self.grammeme_embeddings = None
        self.batch_size = conf["sentence_batch_size"]

    def add_embeddings(self):
        self.word_embeddings = nn.Embedding.from_pretrained(self.data.embeddings)
        self.char_embeddings = nn.Embedding(len(self.data.vocab.vocab['char-index']), self.conf["char_embeddings_dimension"])
        self.grammeme_embeddings = nn.Embedding(self.data.vocab.vocab['grammeme-index'], self.conf["grammeme_embeddings_dimension"])

    def batches(self, start=0):
        """
        Given CustomDataset instance, creates batches of given size
        """

        batch = [self.data[i] for i in range(start, start + self.batch_size)]
        return batch

    def print_batch(self, batch): # for debug purposes
        sentences = [element[0] for element in batch]
        for sentence in sentences:
            for word in sentence:
                word = [self.data.vocab.vocab["index-char"][char] for char in word[1:]]
                print(*word)


def custom_collate(batch):
    """
    Function takes samples given by CustomDataset and creates batches to be fed into the model

    Parameters
    ---------
    batch : list
        A single batch to be collated. Batch consists of tuples (words, labels) generated by CustomDataset
    """
    sentences = [element[0] for element in batch] # sentences is a list of all list of words
    max_sentence_length = max(map(lambda x: len(x), sentences))
    max_word_length = max([max(map(lambda x: len(x), sentence)) for sentence in sentences])
    for words, labels in batch:
        for word in words:
            word += [0] * (max_word_length - len(word)) # id of the pad token must be 0
        words += [[0] * max_word_length] * (max_sentence_length - len(words))

        for grammemes in labels:
            grammemes += [2, 0] # id of the eos token must be 2
    return batch
    # one-hot encoding for chars
    # return torch.utils.data.default_collate(batch)


if __name__ == "__main__":
    configurate()
    main(config)
