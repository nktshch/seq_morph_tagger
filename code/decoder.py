"""decoder.py has class Decoder that uses Encoder outputs to generate grammemes for each word."""

import torch
import torch.nn as nn


class Decoder(nn.Module):
    """Class uses the output of Encoder as an input parameters of the LSTM to predict tags for each word.

    Depending on the mode, uses either labels or previously predicted grammemes as inputs for the next step.

    Args:
        conf (dict): Dictionary with configuration parameters.
        data (CustomDataset): Class instance from dataset.py.
    """

    def __init__(self, conf, data):
        super().__init__()
        self.conf = conf
        self.data = data
        self.grammeme_embeddings = nn.Embedding(len(data.vocab.vocab['grammeme-index']),
                                                self.conf["grammeme_embeddings_dimension"])
        self.grammemeLSTMcell = nn.LSTMCell(input_size=self.conf['grammeme_embeddings_dimension'],
                                            hidden_size=self.conf['grammeme_LSTM_hidden'])
        self.grammemeDropout = nn.Dropout(p=self.conf['grammeme_LSTM_input_dropout'])
        self.linear = nn.Linear(in_features=self.conf['grammeme_LSTM_hidden'],
                                out_features=len(data.vocab.vocab['grammeme-index']))

    def forward(self, labels_batch, decoder_hidden, decoder_cell):
        """Takes batches of hidden and cell states produced by Encoder as the initial states for the LSTM.

        At each timestep feeds either predicted grammeme embedding or known labels as the next inputs.
        Also, perfomes reduction after the prediction to match the number of grammems in the vocabuary.
        Returns predictions (outputs at each timestep) and probabilities that will be used to calculate the loss.

        Args:
            labels_batch (torch.Tensor): Contains labels for each batch.
                When in train mode, labels are fed as inputs at each timestep.
                When in test mode, only SOS tokens are used.
            decoder_hidden (torch.Tensor): The initial hidden state for the decoder. Generated by Encoder.
                Shape (max_sentence_length * batch_size, grammeme_LSTM_hidden).
            decoder_cell (torch.Tensor): The initial cell state for the decoder. Generated by Encoder.
                Shape (max_sentence_length * batch_size, grammeme_LSTM_hidden).
        Returns:
            tuple: Tuple consists of predicted grammemes and probabilities.
                The first tensor has shape (max_grammeme_length, batch_size * max_sentence_length).
                The second tensor has shape (max_grammeme_length, batch_size * max_sentence_length, grammemes_in_vocab)
        """

        # during training, these will be fed one at a time, instead of outputs at each time step
        if labels_batch.shape[0] != 1:
            labels_batch = labels_batch[:-1]
        labels = self.grammeme_embeddings(labels_batch) # embeddings of grammemes,
        # size (max_grammeme_length, batch_size * max_sentence_length, grammeme_embeddings_dimension)
        # size[0] = loop length (sequence length), size[1] = size of the batch, size[3] = input size
        labels = self.grammemeDropout(labels)
        hk = decoder_hidden
        ck = decoder_cell
        predictions = []
        probabilities = []
        if self.training:
            for grammemes in labels:
                hk, ck = self.grammemeLSTMcell(grammemes, (hk, ck))
                probabilities_batch = self.linear(hk)
                probabilities += [probabilities_batch]
                predictions_batch = torch.argmax(probabilities_batch, dim=1)
                predictions += [predictions_batch]

        else:  # using generated grammemes as the next input
            grammemes = labels[0]
            for _ in range(self.conf['decoder_max_iterations']):
                hk, ck = self.grammemeLSTMcell(grammemes, (hk, ck))
                probabilities_batch = self.linear(hk)
                probabilities += [probabilities_batch]
                predictions_batch = torch.argmax(probabilities_batch, dim=1)
                grammemes = self.grammeme_embeddings(predictions_batch)
                predictions += [predictions_batch]

        predictions = torch.stack(predictions)
        probabilities = torch.stack(probabilities)
        # predictions has shape (max_grammeme_length, batch_size * max_sentence_length)
        # probabilities has shape (max_grammeme_length, batch_size * max_sentence_length, grammemes_in_vocab)
        return predictions, probabilities
