"""Contains methods for calculating metrics, collating batches, converting output into strings."""

import torch
from torch.utils.data import Subset


def collate_batch(batch, pad_id=0, sos_id=1, eos_id=2):  # do all preprocessing here
    """Takes batch created with CustomDataset and performs padding.

    batch consists of indices of words, chars, and labels (grammemes). The returned batches are tensors.

    Args:
        batch (list): Single batch to be collated.
            Batch consists of tuples (words, labels) generated by CustomDataset.
        pad_id (int, default 0): The id of the pad token in all dictionaries.
        sos_id (int, default 1): The id of the sos (start of sequence) token.
        eos_id (int, default 2): The id of the eos (end of sequence) token.

    Returns:
        tuple: (words_batch, chars_batch, labels_batch).
            All of them have type torch.Tensor. Size of words_batch is (max_sentence_length, batch_size).
            Size of chars_batch is (batch_size * max_sentence_length, max_word_length). Size of labels_batch is
            (max_label_length, batch_size * max_sentence_length)
    """

    sentences = [element[0] for element in batch]  # sentences is a list of all list of words
    tags = [element[1] for element in batch]  # tags is a list of all lists of grammemes
    max_sentence_length = max(map(lambda x: len(x), sentences))
    max_word_length = max([max(map(lambda x: len(x), sentence)) for sentence in sentences])
    max_label_length = 2 + max([max(map(lambda x: len(x), tag)) for tag in tags])  # +2 because of the sos and eos token
    words_batch = []
    chars_batch = []
    labels_batch = []
    for words, labels in batch:
        words_indices = []
        chars_indices = []
        labels_indices = []
        for word in words:
            word += [pad_id] * (max_word_length - len(word))  # id of the pad token must be 0
            words_indices += [word[0]]
            chars_indices += [word[1:]]
        words_indices += [pad_id] * (max_sentence_length - len(words))
        chars_indices += [[pad_id] * (max_word_length - 1)] * (max_sentence_length - len(words))

        for label in labels:
            label.insert(0, sos_id)  # id of the sos token must be 1
            label += [eos_id]  # id of the eos token must be 2
            label += [pad_id] * (max_label_length - len(label))
            labels_indices += [label]
        labels_indices += [[pad_id] * max_label_length] * (max_sentence_length - len(words))

        words_batch += [words_indices]
        chars_batch += [chars_indices]
        labels_batch += [labels_indices]

    words_batch = torch.tensor(words_batch, dtype=torch.long)
    words_batch = words_batch.transpose(1, 0)
    chars_batch = torch.tensor(chars_batch, dtype=torch.long)
    chars_batch = chars_batch.view(-1, chars_batch.shape[2])
    labels_batch = torch.tensor(labels_batch, dtype=torch.long)
    labels_batch = labels_batch.view(-1, labels_batch.shape[2]).permute(1, 0)
    return words_batch, chars_batch, labels_batch


def subset_from_dataset(data, n):
    """Outputs first n entries from data (type Dataset) as another dataset."""
    return Subset(data, range(n))


def masked_select(a, value):
    """Zero all elements that come after a given value in a row. Used for zeroing elements after EOS token.

    Args:
        a (torch.Tensor): Input tensor.
        value (a.dtype): Value after which all elements should be equal to zero (EOS token index).

    Returns:
        torch.Tensor: Masked tensor of the same shape as a.

    Examples:
        >>> input_tensor = torch.Tensor([[1., 2., 3., 4., 99., 5., 2., 1.],
                              [1., 99., 99., 4., 3., 5., 99., 3.],
                              [1., 3., 3., 4., 1., 5., 2., 1.]])
        >>> print(masked_select(input_tensor, 99))
        tensor([[ 1.,  2.,  3.,  4., 99.,  0.,  0.,  0.],
                [ 1., 99.,  0.,  0.,  0.,  0.,  0.,  0.],
                [ 1.,  3.,  3.,  4.,  1.,  5.,  2.,  1.]])
    """

    mask = []
    rng = torch.arange(0, a.shape[1]).to(config['device'])
    for row in a:
        equal = torch.isin(row, value)
        if equal.nonzero().shape[0] != 0:
            mask.append(torch.le(rng, equal.nonzero()[0]))
        else:
            mask.append(rng)

    mask = torch.stack(mask)
    return a * mask


def predictions_to_grammemes(vocabulary, predictions):
    """Turns indices of predictions produced by decoder into actual grammemes (strings).

    Args:
        vocabulary (dict): Vocabulary from Vocab class.
        predictions (torch.Tensor): 2D Tensor containing indices.

    Returns:
        list: List of lists of predicions.
    """

    tags = []
    for tag_indices in predictions:
        tag = []
        for grammeme_index in tag_indices:
            tag += [vocabulary['index-grammeme'][grammeme_index.item()]]
        tags += [tag]
    return tags


def calculate_accuracy(vocabulary, conf, predictions, targets):
    """Metrics is a ratio of correctly predicted tags to total number of tags.

    All grammemes in a tag must be predicted correctly for it to count as correct.
    """

    n_total, n_correct = 0, 0
    masked_predictions = masked_select(predictions.permute(1, 0),
                                       vocabulary.vocab["grammeme-index"][conf['EOS']])

    for tag, target in zip(masked_predictions, targets.permute(1, 0)):
        if target[0] != 0:
            n_total += 1
            n_correct += int(torch.equal(tag, target))

    return n_correct, n_total
